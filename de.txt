Phân tích Dữ liệu lớn: Đề tài Đồ án Cuối kỳ

Giới thiệu
Các bạn sẽ được yêu cầu thiết kế và triển khai một quy trình phân tích dữ liệu hoàn chỉnh
bằng Apache Spark chạy trên HDFS.
Bạn sẽ chọn một trong ba đề tài được mô tả dưới đây. Mỗi đề tài được thiết kế như một dự án
lớn nhằm kiểm tra khả năng của bạn trong việc xử lý nhập và làm sạch dữ liệu quy mô lớn
(ETL), thực hiện trích xuất đặc trưng phức tạp, và áp dụng các mô hình học máy (machine
learning) phù hợp để giải quyết một vấn đề cụ thể.
Yêu cầu chung & Đánh giá
● Công nghệ sử dụng: Công cụ chính của bạn phải là Apache Spark (sử dụng PySpark
hoặc Scala). Bạn sẽ xử lý dữ liệu được lưu trữ trên HDFS.
● Làm việc nhóm: Các bạn sẽ làm việc theo nhóm đã được phân công. Quy mô của các
bộ dữ liệu này khiến việc hoàn thành cá nhân là vô cùng khó khăn.
● Đánh giá: Mỗi đề tài có một tiêu chí đánh giá cụ thể và khách quan. Bài nộp cuối cùng
của nhóm bạn sẽ được chấm điểm dựa trên một bộ dữ liệu kiểm tra ẩn (hidden test set).
Điều này mô phỏng một kịch bản thực tế, nơi hiệu suất của mô hình được đánh giá trên
dữ liệu chưa từng thấy. Lưu ý quan trọng: Bộ dữ liệu kiểm tra dùng để chấm điểm cuối
cùng sẽ được công bố 12 giờ trước thời hạn nộp bài.
● Sản phẩm nộp: Sản phẩm cuối cùng cho mỗi đề tài sẽ là một tệp dự đoán duy nhất ở
định dạng CSV được chỉ định. Bạn cũng sẽ nộp mã nguồn và một báo cáo ngắn gọn
trình bày chi tiết về phương pháp luận, các lựa chọn trích xuất đặc trưng, và kiến trúc
mô hình cuối cùng của bạn.
Hãy chọn một trong ba đề tài sau:

Đề tài 1: Hệ thống Gợi ý Âm nhạc Cá nhân hóa
Mục tiêu
Xây dựng một hệ thống gợi ý (recommendation engine) có khả năng dự đoán những bài hát
mà một người dùng có khả năng sẽ thêm vào một danh sách phát (playlist). Dựa trên các bài
hát hiện có trong một playlist, mô hình của bạn phải tạo ra một danh sách xếp hạng các bài
hát mới được gợi ý. Đây là một bài toán kinh điển và có giá trị cao trong ngành công nghiệp
streaming âm nhạc.
Bộ dữ liệu: Spotify Million Playlist Dataset
Bạn sẽ sử dụng bộ dữ liệu Spotify Million Playlist Dataset. Bộ dữ liệu này chứa một bộ sưu
tập gồm 1 triệu playlist do người dùng Spotify tạo ra. Nó bao gồm tiêu đề playlist, mô tả, và
danh sách đầy đủ các bài hát cho mỗi playlist.
● Nguồn:
(https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge/dataset
_files)
● Quy mô: 1 triệu playlist, hơn 2 triệu bài hát, và gần 70 triệu nghệ sĩ. Dữ liệu được phân
phối dưới dạng một tập các tệp JSON.
● Thách thức: Số lượng playlist và bài hát khổng lồ tạo ra một ma trận tương tác
playlist-bài hát cực lớn và rất thưa (sparse), biến đây thành một bài toán dữ liệu lớn thực
sự.
Xây dựng bài toán Machine Learning
Đây là một bài toán gợi ý nên được mô hình hóa bằng cách sử dụng lọc cộng tác với phản
hồi ẩn (collaborative filtering with implicit feedback). Sự hiện diện của một bài hát trong
playlist là một tín hiệu ẩn về sở thích của người dùng. Mục tiêu của bạn là dự đoán các bài hát
khác mà người dùng có thể thích, dựa trên hành vi tập thể của tất cả người dùng.
Tiêu chí đánh giá & Định dạng nộp bài
● Tiêu chí: Mô hình của bạn sẽ được đánh giá bằng Mean Average Precision (MAP).
Đây là một tiêu chí dựa trên xếp hạng, đo lường mức độ hiệu quả của mô hình trong việc
đặt các bài hát "chính xác" ở vị trí cao trong danh sách gợi ý.
● Định dạng nộp bài: Bạn sẽ nộp một tệp CSV duy nhất có tên submission.csv với hai
cột: playlist_id và recommended_track_uris. Cột thứ hai phải chứa một danh sách 500
URI của các bài hát được gợi ý, được phân tách bằng dấu phẩy, và sắp xếp theo thứ tự
từ tự tin nhất đến ít tự tin nhất.